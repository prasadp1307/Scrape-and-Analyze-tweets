{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cbd50d6",
   "metadata": {},
   "source": [
    "# Scrape and Analyze Tweets\n",
    "    \n",
    "Author: Prasad Patharvat\n",
    "    \n",
    "        What is Web scraping?\n",
    "            In today’s competitive world everybody is looking for ways to innovate and make use of new technologies. \n",
    "            Web scraping (also called web data extraction or data scraping) provides a solution for those who want\n",
    "            to get access to structured web data in an automated fashion. Web scraping is useful if the public website\n",
    "            you want to get data from doesn’t have an API, or it does but provides only limited access to the data.\n",
    "\n",
    "        \n",
    "\n",
    "        snscrape is a scraper for social networking services (SNS). \n",
    "        It scrapes things like user profiles, hashtags, or searches and returns the discovered items,\n",
    "        e.g. the relevant posts.\n",
    "\n",
    "        The following services are currently supported:\n",
    "\n",
    "            Facebook: user profiles, groups, and communities (aka visitor posts)\n",
    "            Twitter : user profiles, hashtags, and location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760adfaa",
   "metadata": {},
   "source": [
    "#    Algorithms and Libraries Used : \n",
    "Time\n",
    "snsrape\n",
    "pandas\n",
    "tqdm\n",
    "\n",
    "       Time :  As the name suggests Python time module allows to work with time in Python.\n",
    "               It allows functionality like getting the current time,pausing the Program \n",
    "               from executing, etc. So before starting with this module we need to import it.\n",
    "            \n",
    "       snscrape: snscrape is a scraper for social networking services (SNS). \n",
    "                  It scrapes things like user profiles, hashtags, or searches and returns the discovered items,\n",
    "                  e.g. the relevant posts. \n",
    "                    \n",
    "       Pandas: Pandas is an open source Python package that is most widely used for data science/data analysis\n",
    "               and machine learning tasks. It is built on top of another package named Numpy, which provides support \n",
    "               for multi-dimensional arrays   \n",
    "        \n",
    "        \n",
    "       tqdm:   tqdm is a library in Python which is used for creating Progress Meters \n",
    "               or Progress Bars. tqdm got its name from the Arabic name taqaddum which means 'progress'.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eb2547",
   "metadata": {},
   "source": [
    "# Important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a255da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import snscrape.modules.twitter as sntwitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b07f60ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c5c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68780b7",
   "metadata": {},
   "source": [
    "# User Defined Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d919cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Twitter account keyword: flipkart\n",
      "How many tweets do you want fetch: 1000\n"
     ]
    }
   ],
   "source": [
    "username = input('Enter your Twitter account keyword: ')\n",
    "number = int(input('How many tweets do you want fetch: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622b07d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a35a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tweets in enumerate(sntwitter.TwitterSearchScraper('{}'.format(username)).get_items()):\n",
    "    if i > number:\n",
    "        break\n",
    "    tweet_data.append([tweets.date, tweets.content, tweets.user.username, tweets.url])    \n",
    "    \n",
    "df = pd.DataFrame(tweet_data, columns=['Date', 'Tweets', 'Username','Url'])\n",
    "df.to_csv(f'{username}.csv',index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc0f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb3a1bda",
   "metadata": {},
   "source": [
    "# Challenges  Faced:\n",
    "                \n",
    "            The most popular scraping internet data challenge. Websites can decide whether they will give bots access to clean data. Some sites forbid automatic data collection. The reasons for the ban can be completely different. If you come across a website that prohibits collection through its robots.txt, follow fair play principles and ask the site owner for permission to collect data. Otherwise, it is better to look for an alternative site with similar information. \n",
    "            \n",
    "            Websites may be slow to load content or may not load at all when receiving a large number of access requests. In such a situation, you can refresh the page and wait for the site to recover. However, the parser will not know how to handle such a situation and data collection may be interrupted. \n",
    "            \n",
    "            Another website challenge you have to face when scraping. Designers may have their design standards when creating web pages, so page structures will vary. Websites also undergo periodic changes to improve user interaction or add new features. This often results in structural changes to the web page itself. Web parsers are created with page code elements in mind, so these changes make the codes more complex, which affects how the parsers work.\n",
    "\n",
    "    And because they are customized to a specific page design, they won’t work for the updated page. Sometimes even a minor change requires a new parser configuration.\n",
    "    \n",
    "    System Breakdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9443d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e1d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336e2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
